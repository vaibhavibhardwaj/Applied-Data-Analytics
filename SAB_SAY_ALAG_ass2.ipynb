{"cells":[{"metadata":{},"cell_type":"markdown","source":"# FIT5149 S2 2019 Ass2 Report                                 \n#### Group No. 34\n\n### SCIENTIFIC DOCUMENT CLASSIFICATION\n\n\n\n>   Vaibhavi Bhardwaj - 30154987\n\n>  Hassaan Ali Khan - 30703700\n\n>  Gayatri Aniruddha - 30945305\n"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"cell_type":"markdown","source":"## Libraries"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport re\nimport os\nimport matplotlib.pylab as plt\n\n%matplotlib inline\nimport math  \nimport glob\nfrom numpy import mean\nfrom numpy import std\nfrom matplotlib import pyplot\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn import model_selection, naive_bayes, svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\nfrom scipy.sparse import hstack\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Read Train and Test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading the dataset needed to tarin and test the data\n\n\ntrain = pd.read_csv('../train_data_labels.csv')\ntest = pd.read_csv('../test_data.csv')\n\n# dropping the null rows \ntrain['abstract'].dropna(inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# loading train and test abstract  \ntrain_text = train['abstract']\ntest_text = test['abstract']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing of Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Code for preprocessing and lemmitization\n# function to take data with lenght greater than or equal to 3 \n# lowering the word characters \ndef tokenizeRawData(word):\n    tokenizer_rawdata = RegexpTokenizer(r'\\w{3,}')\n    word = tokenizer_rawdata.tokenize(word.lower())\n    return (word)\n\n\n# making a list of english stop words to be removed \nstopwords_list = stopwords.words('english')\n\n# function to remove stop words for a list \n# list to be changed is word_list\ndef stopwords_function(word_list):\n    # removing stop words from the list\n    new_list_without_stopwords = [w for w in word_list if w not in stopwords_list]\n    # returning the list to be preinted \n    return new_list_without_stopwords      \n\n\n# class to clean and tokensie the list \n# the words in the list will be cleaned \nclass LemmaTokenizer(object):\n    def __init__(self):\n        # calling the word lemmatizer\n        self.wnl=WordNetLemmatizer()\n \n    def __call__(self,document):\n        \n        # calling the tokenize to tokenise greater than length 2 words \n        document = tokenizeRawData(document)\n        # removing stop words from the list \n        document = stopwords_function(document)\n        # calling \n        return [self.wnl.lemmatize(t) for t in document]\n\n#Reference\n# https://stackoverflow.com/questions/47423854/sklearn-adding-lemmatizer-to-countvectorizer","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Defining TFID Vectroizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"#TFid vectroizer used to extract features of and auto preprocesses data for us\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True, \n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{3,}',\n    ngram_range=(1, 3), # unigrams to trigrams \n    min_df=0.01,\n    max_df=0.99,\n    max_features=50000, # number of max features \n    tokenizer = LemmaTokenizer() # word tokenizer \n    )\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Splitting Data "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Splitting data for test and tarin\n# 30 percent of the data is for validation\nTrain_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(train['abstract'],\n                                                                    train['label'],\n                                                                    test_size=0.3,\n                                                                    random_state=0,\n                                                                    stratify=train['label'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Fitting to Vectroizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"# fitting the data to the word vectorizer\nword_vectorizer.fit(train_text)\n\n# transforming the data for test validation and train \ntrain_word_features = word_vectorizer.transform(Train_X)\ntest_x_word_features = word_vectorizer.transform(Test_X)\n\n# tranforming for test data \ntest_word_features = word_vectorizer.transform(test_text)\n\n# traning making a stack for training , test and validation test\ntrain_features = hstack([train_word_features])\ntest_x_features = hstack([test_x_word_features])\ntest_features = hstack([test_word_features])\n\nscores = []\nsubmission = pd.DataFrame.from_dict({'test_id': test['test_id']})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Encoding Labels"},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# calling the label encoder to assign interger values to 1 to 100 to the labels\nle = LabelEncoder()\n# fitting the training data \nTrain_Y_encode = le.fit_transform(Train_Y)\n# tranforming the test lables \nTest_Y_encode = le.fit_transform(Test_Y)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Cross Validation and Score Prediction on Validation dataset "},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluating dataset \ndef evaluating_model(X_train, y_train, X_test, y_test, model):\n    \n    # define evaluation procedure\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n    \n    # evaluate model\n    scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n    \n    model.fit(X_train, y_train)\n    \n    predictions = models[i].predict(X_test)\n    \n    accuracy = accuracy_score(predictions, y_test)\n    \n    return scores,accuracy,predictions\n \n\n# the final model for evaluation     \ndef get_modeling():\n    model, name = list(), list()\n    \n    # running LR as that's the final model\n    model.append(LogisticRegression(solver='saga', max_iter=1000))\n    name.append('LR-saga')\n        \n    return model, name\n \n    \n# calling get modling for the \nmodels, names = get_modeling()\n\n# evaluate each model\nfor i in range(len(models)):\n    # evaluate the model and store results\n    scores, accuracy, prediction = evaluating_model(train_features, Train_Y_encode, test_x_word_features, Test_Y_encode, models[i])\n    \n    # summarize performance\n    print('>%s Train Score: %.3f (%.3f) Test Score: %.3f (%.3f)' % (names[i], mean(scores), std(scores), mean(accuracy), std(accuracy)))\n    \n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction of Complete Train Dataset on Test set "},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# making a model for prediction\nmodel = LogisticRegression(solver='saga', max_iter=1000) \n\n# Defining features on train\ntrain_word_features1 = word_vectorizer.transform(train['abstract'])\ntrain_features1 = hstack([train_word_features1])\n\nTrain_encode = le.fit_transform(train['label'])\n\n\n# fitting a model for training and testing \nmodel.fit(train_features1, Train_encode)\n# predicting the values \ny_preds = model.predict(test_features)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Converting to CSV Format"},{"metadata":{"trusted":true},"cell_type":"code","source":"# converting labels to final values \nlabel = le.inverse_transform(y_preds)\n\n# ASSIGNING the dataset to a dummy dataframe \ntest1 = test \n\n# removing abstrtact from the dummy dataset\ntest1 = test1.drop(columns=['abstract'])\n# assigning a column 'label'\ntest1['label'] = 'null'\n\n# making the final data with test results\n# test_id, label as the result for the dataset\nfor index, row in test1.iterrows():\n    test1.iloc[index,test1.columns.get_loc('label')] = label[index]\n    \n# writing to the csv file \ntest1.to_csv('pred_labels.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}